"""
Servicios de IA para generaci√≥n de contenido de blog.
Refactorizado para usar LangChain y LangGraph.
"""
import os
import json
import re
from typing import List
from dotenv import load_dotenv

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser

from .llm_config import get_llm
from .schemas import ResearchResult, SeoMetadataSchema
from .research_graph import research_app

load_dotenv()

# --- MODEL CONFIG ---
# Usamos un valor por defecto para el modelo de texto, pero permitimos override
DEFAULT_TEXT_MODEL = "gemini-2.0-flash"


# --- FUNCIONES REFACTORIZADAS ---

def research_topic(topic: str, model: str = DEFAULT_TEXT_MODEL) -> dict:
    """
    Investiga un tema usando el Agente de Investigaci√≥n Profunda (LangGraph).
    
    Args:
        topic: El tema a investigar
        model: El modelo de IA a usar (se pasa el nombre al grafo)
        
    Returns:
        dict con puntos_clave, preguntas_frecuentes, keywords_seo
    """
    try:
        # Ejecutamos el grafo
        inputs = {"topic": topic}
        
        # Invocamos el agente
        # Nota: El agente internamente usa get_llm, que lee el modelo por defecto.
        # Si queremos pasar el modelo din√°micamente, deber√≠amos pasarlo en el state 
        # o configurar el grafo para aceptarlo. Por ahora usamos la config del grafo.
        
        result = research_app.invoke(inputs)
        
        # El resultado final est√° en el estado 'final_report'
        if "final_report" in result and result["final_report"]:
            return result["final_report"]
        else:
            return _fallback_research(topic, model)

    except Exception as e:
        print(f"Error en LangGraph research: {e}")
        return _fallback_research(topic, model)


def _fallback_research(topic: str, model: str) -> dict:
    """Fallback usando una cadena simple si el grafo falla."""
    print("Usando fallback research...")
    llm = get_llm(model_name=model)
    structured_llm = llm.with_structured_output(ResearchResult)
    
    prompt = f"Investiga el tema: '{topic}'. Act√∫a como experta en salud familiar."
    
    try:
        result = structured_llm.invoke(prompt)
        return result.dict()
    except Exception as e:
         return {
            "puntos_clave": [f"Error al procesar la investigaci√≥n: {str(e)}"],
            "preguntas_frecuentes": [],
            "keywords_seo": []
        }


def generate_blog_draft(topic: str, tone: str = "profesional y emp√°tico", model: str = DEFAULT_TEXT_MODEL) -> str:
    """
    Genera un borrador completo de blog usando LangChain.
    """
    llm = get_llm(model_name=model)
    
    template = """
    Eres la Dra. Lina, una reconocida especialista en salud familiar.
    Escribe un art√≠culo de blog completo sobre: "{topic}"
    
    Requisitos:
    - Extensi√≥n: 800-1200 palabras
    - Tono: {tone}. El texto debe ser f√°cil de leer, entretenido y fluido.
    - Enfoque: Trata temas de salud general y familiar.
    - Formato: HTML con etiquetas <h2>, <h3>, <p>, <ul>, <li>
    - Incluir una introducci√≥n muy atractiva (hook).
    - Desarrollar 3-4 secciones principales con subt√≠tulos.
    - Incluir consejos pr√°cticos y aplicables.
    - Terminar con una conclusi√≥n memorable y un llamado a la acci√≥n.
    - Optimizado para SEO y experiencia de usuario.
    
    NO incluir etiquetas <html>, <head>, <body> ni <h1>.
    Start directly with the content.
    """
    
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | llm
    
    try:
        response = chain.invoke({"topic": topic, "tone": tone})
        return response.content
    except Exception as e:
        return f"<p>Error al generar contenido: {str(e)}</p>"


def generate_seo_metadata(title: str, content: str) -> dict:
    """
    Genera metadatos SEO usando StructuredOutput de LangChain.
    """
    # Truncar contenido para el prompt
    content_preview = content[:2000] if len(content) > 2000 else content
    
    llm = get_llm(model_name=DEFAULT_TEXT_MODEL)
    structured_llm = llm.with_structured_output(SeoMetadataSchema)
    
    prompt = f"""
    Genera metadatos SEO para este art√≠culo:
    T√≠tulo: {title}
    Contenido (preview): {content_preview}
    """
    
    try:
        result = structured_llm.invoke(prompt)
        return result.dict()
    except Exception as e:
        return {
            "meta_description": "",
            "keywords": [],
            "slug_sugerido": ""
        }


def refine_block_content(content: str, action: str, context: str = "", model: str = DEFAULT_TEXT_MODEL) -> str:
    """
    Refina el contenido usando LangChain.
    """
    llm = get_llm(model_name=model)
    
    action_prompts = {
        "expand": "Expande este p√°rrafo con m√°s detalles (2-3 p√°rrafos extra).",
        "shorten": "Resume este texto en 1-2 oraciones claras.",
        "formal": "Reescribe con un tono formal y m√©dico profesional.",
        "casual": "Reescribe con un tono cercano y f√°cil de entender.",
        "scientific": "A√±ade enfoque cient√≠fico y datos precisos."
    }
    
    instruction = action_prompts.get(action, "Mejora este texto.")
    
    template = """
    Eres la Dra. Lina.
    Contexto: {context}
    
    Texto original:
    "{content}"
    
    Instrucci√≥n: {instruction}
    Devuelve SOLO el texto reescrito.
    """
    
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | llm
    
    try:
        response = chain.invoke({
            "content": content, 
            "context": context, 
            "instruction": instruction
        })
        return response.content.strip()
    except Exception as e:
        return f"Error: {str(e)}"


def generate_featured_image(title: str, model: str = "dall-e-3") -> str:
    """
    Genera imagen. Mantenemos la l√≥gica original pero usamos los clientes centralizados donde sea posible.
    Para DALL-E, LangChain tiene wrapper pero para generaci√≥n de imagen directa a veces es mejor la API directa si devuelve URL.
    LangChain 'DallEAPIWrapper' existe, pero para mantener consistencia con el c√≥digo anterior de guardado local (Gemini) 
    vs URL (DALL-E), adaptaremos ligeramente.
    """
    
    prompt_text = f"""
    Imagen profesional y c√°lida para un art√≠culo m√©dico sobre: "{title}".
    Estilo: Fotograf√≠a editorial suave, iluminaci√≥n natural y colores c√°lidos.
    Tema: Salud familiar integral, crianza, bienestar.
    Sin texto. Sin rostros definidos.
    """
    
    try:
        if "gemini" in model.lower():
            # Usamos el cliente de Gemini via Google GenAI SDK directo porque LangChain ChatGoogleGenerativeAI 
            # est√° enfocado en chat/texto, aunque soporta multimodal input, output de imagen es diferente.
            # Podr√≠amos instanciar el cliente aqu√≠ o exponerlo en llm_config. Importaremos directo por compatibilidad.
            from google import genai
            
            api_key = os.environ.get("GEMINI_API_KEY")
            client = genai.Client(api_key=api_key)
            
            response = client.models.generate_content(
                model=model if "flash" in model else "gemini-2.0-flash",
                contents=prompt_text,
                config={'response_mime_type': 'image/png'}
            )
            
            # Procesar imagen (l√≥gica original)
            image_data = None
            if response.parts:
                for part in response.parts:
                    if part.inline_data:
                        image_data = part.inline_data.data
                        break
                        
            if not image_data:
                return "error: No se recibi√≥ imagen de Gemini."

            # Guardar
            import uuid
            from datetime import datetime
            
            filename = f"gen_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:6]}.png"
            save_path = os.path.join("static", "generated_images", filename)
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            with open(save_path, "wb") as f:
                f.write(image_data)
                
            return f"/static/generated_images/{filename}"

        else:
            # OpenAI DALL-E 3
            # Podemos usar langchain_community.utilities.dalle_image_generator.DallEAPIWrapper
            # o mantener 'openai' directo. Mantendremos openai directo para no depender de langchain_community por ahora
            # si solo tenemos langchain-openai.
            from openai import OpenAI
            client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
            
            response = client.images.generate(
                model="dall-e-3",
                prompt=prompt_text,
                size="1024x1024",
                quality="standard",
                n=1,
            )
            return response.data[0].url

    except Exception as e:
        return f"error:{str(e)}"


def analyze_seo(title: str, content: str, keywords: list) -> dict:
    """
    Mantenemos la l√≥gica original de an√°lisis SEO por ahora, ya que es determinista y r√°pida.
    Podr√≠amos moverla a un agente 'Reviewer' en el futuro.
    """
    # ... (Copiar l√≥gica original o importarla si la separamos)
    # Para simplicidad, pego la l√≥gica original de conteo de palabras
    
    clean_content = re.sub(r'<[^>]+>', ' ', content).lower()
    word_count = len(clean_content.split())
    
    issues = []
    score = 100
    
    if word_count < 300:
        issues.append("‚ö†Ô∏è Contenido muy corto (< 300 palabras)")
        score -= 20
    elif word_count < 600:
        issues.append("üí° Considera expandir a 600+ palabras")
        score -= 10
    
    if len(title) < 30:
        issues.append("‚ö†Ô∏è T√≠tulo muy corto")
        score -= 10
    
    keyword_presence = {}
    for kw in keywords[:5]:
        kw_lower = kw.lower()
        count = clean_content.count(kw_lower)
        keyword_presence[kw] = count
        if count == 0:
            issues.append(f"‚ùå Keyword '{kw}' no encontrada")
            score -= 10
    
    return {
        "score": max(0, score),
        "issues": issues,
        "word_count": word_count,
        "keyword_presence": keyword_presence
    }
