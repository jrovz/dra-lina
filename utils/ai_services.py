"""
Servicios de IA para generaci√≥n de contenido de blog.
Utiliza Gemini para texto y OpenAI DALL-E 3 para im√°genes.

SDK: google-genai v1.60+ (nuevo SDK 2024)
"""
import os
import json
import re
from dotenv import load_dotenv

load_dotenv()

# Clientes de IA (lazy loading)
_openai_client = None
_gemini_client = None
_last_openai_key = None
_last_gemini_key = None

# Modelo de Gemini a usar
GEMINI_MODEL = "gemini-2.0-flash"


def _get_openai_client():
    """Obtiene el cliente de OpenAI (lazy loading con recarga si cambia la key)."""
    global _openai_client, _last_openai_key
    current_key = os.environ.get("OPENAI_API_KEY", "")
    
    if _openai_client is None or current_key != _last_openai_key:
        import openai
        _openai_client = openai.OpenAI(api_key=current_key)
        _last_openai_key = current_key
    return _openai_client


def _get_gemini_client():
    """Obtiene el cliente de Gemini usando el nuevo SDK google-genai."""
    global _gemini_client, _last_gemini_key
    current_key = os.environ.get("GEMINI_API_KEY", "")
    
    if not current_key:
        raise ValueError("GEMINI_API_KEY no est√° configurada")
    
    if _gemini_client is None or current_key != _last_gemini_key:
        from google import genai
        _gemini_client = genai.Client(api_key=current_key)
        _last_gemini_key = current_key
    return _gemini_client


def _generate_text(prompt: str, model: str = "gemini-2.0-flash") -> str:
    """
    Genera texto usando el modelo seleccionado.
    Soporta tanto Gemini como OpenAI GPT.
    """
    if model.startswith("gpt-"):
        # Usar OpenAI
        client = _get_openai_client()
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        return response.choices[0].message.content
    else:
        # Usar Gemini
        client = _get_gemini_client()
        response = client.models.generate_content(
            model=model,
            contents=prompt
        )
        return response.text


def research_topic(topic: str, model: str = "gemini-2.0-flash") -> dict:
    """
    Investiga un tema y devuelve puntos clave para el blog.
    
    Args:
        topic: El tema a investigar
        model: El modelo de IA a usar
        
    Returns:
        dict con puntos_clave, preguntas_frecuentes, keywords_seo
    """
    prompt = f"""
    Eres una experta especialista en salud familiar.
    Investiga el tema: "{topic}"
    
    Devuelve √öNICAMENTE un JSON v√°lido (sin markdown, sin ```json) con esta estructura:
    {{
        "puntos_clave": ["punto 1", "punto 2", "punto 3", "punto 4", "punto 5"],
        "preguntas_frecuentes": ["pregunta 1", "pregunta 2", "pregunta 3"],
        "keywords_seo": ["keyword1", "keyword2", "keyword3", "keyword4", "keyword5"]
    }}
    """
    
    try:
        text = _generate_text(prompt, model=model)
        text = text.strip()
        # Limpiar posibles marcadores de c√≥digo
        text = re.sub(r'^```json\s*', '', text)
        text = re.sub(r'\s*```$', '', text)
        return json.loads(text)
    except json.JSONDecodeError:
        return {
            "puntos_clave": ["Error al procesar la investigaci√≥n"],
            "preguntas_frecuentes": [],
            "keywords_seo": []
        }
    except Exception as e:
        return {
            "error": str(e),
            "puntos_clave": [],
            "preguntas_frecuentes": [],
            "keywords_seo": []
        }


def generate_blog_draft(topic: str, tone: str = "profesional y emp√°tico", model: str = "gemini-2.0-flash") -> str:
    """
    Genera un borrador completo de blog usando IA.
    
    Args:
        topic: El tema del art√≠culo
        tone: El tono deseado para el contenido
        model: El modelo de IA a usar
        
    Returns:
        str: Contenido HTML del art√≠culo
    """
    prompt = f"""
    Eres la Dra. Lina, una reconocida especialista en salud familiar.
    Escribe un art√≠culo de blog completo sobre: "{topic}"
    
    Requisitos:
    - Extensi√≥n: 800-1200 palabras
    - Tono: Formal y muy respetuoso, pero dise√±ado para maximizar el engagement. El texto debe ser f√°cil de leer, entretenido y fluido, captando la atenci√≥n del lector desde la primera l√≠nea.
    - Enfoque: Trata temas de salud general y familiar (no solo ginecolog√≠a).
    - Formato: HTML con etiquetas <h2>, <h3>, <p>, <ul>, <li>
    - Incluir una introducci√≥n muy atractiva (hook).
    - Desarrollar 3-4 secciones principales con subt√≠tulos.
    - Incluir consejos pr√°cticos y aplicables.
    - Terminar con una conclusi√≥n memorable y un llamado a la acci√≥n.
    - Optimizado para SEO y experiencia de usuario (lectura escaneable).
    
    NO incluir etiquetas <html>, <head>, <body> ni <h1>.
    Empezar directamente con el contenido del art√≠culo.
    """
    
    try:
        return _generate_text(prompt, model=model)
    except Exception as e:
        return f"<p>Error al generar contenido: {str(e)}</p>"


def generate_featured_image(title: str, model: str = "dall-e-3") -> str:
    """
    Genera una imagen destacada para el blog usando el modelo seleccionado.
    
    Args:
        title: El t√≠tulo del art√≠culo
        model: El modelo a usar ("dall-e-3" o "gemini-...")
        
    Returns:
        str: URL de la imagen generada (remota o local)
    """
    prompt = f"""
    Imagen profesional y c√°lida para un art√≠culo m√©dico sobre: "{title}".
    
    Estilo: Fotograf√≠a editorial suave, iluminaci√≥n natural y colores c√°lidos.
    Tema: Salud familiar integral, crianza, bienestar, o estilo de vida saludable (incluyendo padres, madres, ni√±os).
    Requisitos: NO incluir texto. NO mostrar rostros definidos (usar desenfoque, de espaldas, o detalles).
    Ambiente: Luminoso, acogedor, transmitiendo calma y uni√≥n familiar.
    """
    
    try:
        if model.lower().startswith("gemini"):
            # Generaci√≥n con Gemini (Imagen 3)
            client = _get_gemini_client()
            
            # Nota: El modelo de imagen de Gemini suele ser 'gemini-2.0-flash' o espec√≠fico como 'imagen-3.0-generate-001'
            # Asumimos que el modelo pasado es capaz de generar im√°genes o usamos el default
            model_to_use = model if "flash" in model else "gemini-2.0-flash"

            response = client.models.generate_content(
                model=model_to_use,
                contents=prompt,
                config={'response_mime_type': 'image/png'} 
            )
            
            # Buscar la parte de imagen en la respuesta
            image_data = None
            if response.parts:
                for part in response.parts:
                    if part.inline_data:
                        image_data = part.inline_data.data
                        break
            
            # Si no hay inline_data, intentamos ver si el texto devolvi√≥ error o algo inesperado
            if not image_data:
                 return "error: No se recibi√≥ imagen de Gemini."

            # Guardar imagen localmente
            import base64
            import uuid
            from datetime import datetime
            
            # Gemini devuelve bytes crudos o base64 dependiendo del SDK, google-genai suele manejarlo internamente
            # En response.parts[0].inline_data.data vienen los bytes
            
            filename = f"gen_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:6]}.png"
            save_path = os.path.join("static", "generated_images", filename)
            
            # Asegurar directorio
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            with open(save_path, "wb") as f:
                f.write(image_data)
                
            return f"/static/generated_images/{filename}"

        else:
            # Generaci√≥n con OpenAI DALL-E 3
            client = _get_openai_client()
            response = client.images.generate(
                model="dall-e-3",
                prompt=prompt,
                size="1024x1024",
                quality="standard",
                style="natural",
                n=1,
            )
            return response.data[0].url

    except Exception as e:
        return f"error:{str(e)}"


def generate_seo_metadata(title: str, content: str) -> dict:
    """
    Genera metadatos SEO para el art√≠culo.
    
    Args:
        title: T√≠tulo del art√≠culo
        content: Contenido del art√≠culo
        
    Returns:
        dict con meta_description, keywords, slug_sugerido
    """
    # Truncar contenido para el prompt
    content_preview = content[:1000] if len(content) > 1000 else content
    
    prompt = f"""
    Genera metadatos SEO para este art√≠culo:
    T√≠tulo: {title}
    Contenido (preview): {content_preview}
    
    Devuelve √öNICAMENTE un JSON v√°lido (sin markdown) con:
    {{
        "meta_description": "descripci√≥n de 150-160 caracteres",
        "keywords": ["keyword1", "keyword2", "keyword3"],
        "slug_sugerido": "url-amigable-del-articulo"
    }}
    """
    
    try:
        text = _generate_text(prompt)
        text = text.strip()
        text = re.sub(r'^```json\s*', '', text)
        text = re.sub(r'\s*```$', '', text)
        return json.loads(text)
    except:
        return {
            "meta_description": "",
            "keywords": [],
            "slug_sugerido": ""
        }


def refine_block_content(content: str, action: str, context: str = "", model: str = "gemini-2.0-flash") -> str:
    """
    Refina el contenido de un bloque espec√≠fico usando IA.
    
    Args:
        content: El texto del bloque a refinar
        action: La acci√≥n a realizar (expand, shorten, formal, casual, scientific)
        context: Contexto adicional (t√≠tulo del art√≠culo, etc.)
        model: El modelo de IA a usar
        
    Returns:
        str: El contenido refinado
    """
    action_prompts = {
        "expand": """
            Expande este p√°rrafo con m√°s detalles, ejemplos y explicaciones.
            Mant√©n el mismo tono y estilo. Genera 2-3 p√°rrafos adicionales.
            Devuelve SOLO el texto expandido, sin etiquetas HTML.
        """,
        "shorten": """
            Resume este texto de forma concisa, manteniendo las ideas principales.
            Reduce a 1-2 oraciones claras y directas.
            Devuelve SOLO el texto resumido.
        """,
        "formal": """
            Reescribe este texto con un tono m√°s formal y profesional.
            Usa vocabulario t√©cnico apropiado para contenido m√©dico.
            Devuelve SOLO el texto reescrito.
        """,
        "casual": """
            Reescribe este texto con un tono m√°s cercano y emp√°tico.
            Hazlo f√°cil de entender para pacientes no especializadas.
            Devuelve SOLO el texto reescrito.
        """,
        "scientific": """
            Reescribe con enfoque cient√≠fico, a√±adiendo datos o estad√≠sticas relevantes.
            Mant√©n la precisi√≥n m√©dica.
            Devuelve SOLO el texto reescrito.
        """
    }
    
    base_prompt = action_prompts.get(action, action_prompts["expand"])
    
    full_prompt = f"""
    Eres la Dra. Lina, especialista en salud familiar.
    {f'Contexto del art√≠culo: {context}' if context else ''}
    
    Texto original:
    "{content}"
    
    {base_prompt}
    """
    
    try:
        return _generate_text(full_prompt, model=model).strip()
    except Exception as e:
        return f"Error: {str(e)}"



def analyze_seo(title: str, content: str, keywords: list) -> dict:
    """
    Analiza el SEO del contenido actual.
    
    Args:
        title: T√≠tulo del art√≠culo
        content: Contenido HTML del art√≠culo
        keywords: Lista de keywords objetivo
        
    Returns:
        dict con score, issues, y sugerencias
    """
    # Limpiar HTML para an√°lisis
    clean_content = re.sub(r'<[^>]+>', ' ', content)
    clean_content = clean_content.lower()
    word_count = len(clean_content.split())
    
    issues = []
    score = 100
    
    # Verificar longitud
    if word_count < 300:
        issues.append("‚ö†Ô∏è Contenido muy corto (< 300 palabras)")
        score -= 20
    elif word_count < 600:
        issues.append("üí° Considera expandir a 600+ palabras")
        score -= 10
    
    # Verificar t√≠tulo
    if len(title) < 30:
        issues.append("‚ö†Ô∏è T√≠tulo muy corto")
        score -= 10
    elif len(title) > 60:
        issues.append("‚ö†Ô∏è T√≠tulo muy largo para SEO")
        score -= 5
    
    # Verificar keywords
    keyword_presence = {}
    for kw in keywords[:5]:  # Analizar top 5 keywords
        kw_lower = kw.lower()
        count = clean_content.count(kw_lower)
        keyword_presence[kw] = count
        if count == 0:
            issues.append(f"‚ùå Keyword '{kw}' no encontrada")
            score -= 10
        elif count < 2:
            issues.append(f"üí° Usa m√°s '{kw}' (actualmente: {count})")
            score -= 5
    
    # Verificar estructura HTML
    has_h2 = '<h2' in content.lower()
    has_h3 = '<h3' in content.lower()
    has_list = '<ul' in content.lower() or '<ol' in content.lower()
    
    if not has_h2:
        issues.append("‚ùå Falta encabezado H2")
        score -= 15
    if not has_h3:
        issues.append("üí° A√±adir subt√≠tulos H3")
        score -= 5
    if not has_list:
        issues.append("üí° A√±adir listas para mejor lectura")
        score -= 5
    
    score = max(0, min(100, score))
    
    return {
        "score": score,
        "word_count": word_count,
        "issues": issues,
        "keyword_presence": keyword_presence,
        "structure": {
            "has_h2": has_h2,
            "has_h3": has_h3,
            "has_list": has_list
        }
    }
